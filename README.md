# local-llm-example
OpenUI Demo to run a local LLM using ollama on an RTX3050.

https://docs.ollama.com/quickstart

https://github.com/open-webui/open-webui
