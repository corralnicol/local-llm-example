# local-llm-example
OpenUI Demo to run a local LLM using ollama on an RTX3050.
